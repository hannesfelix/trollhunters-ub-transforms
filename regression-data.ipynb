{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! pip install --quiet langdetect\n",
    "! pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Retweet Regression Data\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/work/gcs-connector-hadoop2-latest.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/home/jovyan/work/key.json\") \\\n",
    "    .config(\"spark.driver.memory\", \"22g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tweets = spark.read.parquet('gs://spain-tweets-warehouse')\n",
    "\n",
    "tweets = tweets \\\n",
    "    .where(tweets.datestamp >= datetime(2017,8,1)) \\\n",
    "    .where(tweets.datestamp < datetime(2017,8,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tweets.createOrReplaceTempView('tweets')\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from itertools import permutations\n",
    "\n",
    "def confident_lang(text):\n",
    "    try: \n",
    "        langs = detect_langs(text)\n",
    "        top = langs[0]\n",
    "        if top.prob > 0.75:\n",
    "            return top.lang\n",
    "        elif top.lang == 'cat' or top.lang == 'es':\n",
    "            # print(f'could not find language.\\n Probs: {langs}.\\n Text: {text}')\n",
    "            return None\n",
    "        else:\n",
    "            return None\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def user_lang(di):\n",
    "    \"\"\"Picks the language of the user\n",
    "\n",
    "    :param di: dictionary of language-> percentage\n",
    "    :returns: language picked for user, as a string\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> user_lang({ 'es': .7, 'en': .3 })\n",
    "    es\n",
    "\n",
    "    \"\"\"\n",
    "    if di.get('ca', 0) > 0.10:\n",
    "        return 'ca'\n",
    "    \n",
    "    lang,val = None,0\n",
    "\n",
    "    for k,v in di.items():\n",
    "        if v > val:\n",
    "            lang,val = k,v\n",
    "\n",
    "    return lang\n",
    "\n",
    "def create_user(di):\n",
    "    lang = user_lang(di['langs'])\n",
    "    return (lang, di['retweets'])\n",
    "\n",
    "\n",
    "def user_stats(user_info, user_networks, users, user):\n",
    "\n",
    "    # this allows network to be decoupled from this stage\n",
    "    network = user_networks[user]\n",
    "    relevant = [(u, user_info[u]) for u in users if u in network]\n",
    "\n",
    "    get_lang = lambda lang: [id_ for id_, (lang, _) in relevant\n",
    "                             if lang == lang]\n",
    "\n",
    "    # compute wanted stats:\n",
    "    tot_engaged = len(relevant)\n",
    "    tot_cat_engaged = len(get_lang('cat'))\n",
    "    net_size = len(network)\n",
    "    user_lang, _ = user_info[user]\n",
    "\n",
    "    return user, user_lang, tot_engaged, tot_cat_engaged, net_size\n",
    "\n",
    "\n",
    "def get_stats_for_users(user_info, user_networks, tweet, lang, users):\n",
    "    stats = [user_stats(user_info, user_networks, users, user)\n",
    "             for user in users]\n",
    "\n",
    "    # TODO: add tweet language...\n",
    "    stats = [(tweet, lang) + s for s in stats]\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: not getting extended text!!!\n",
    "# text should not be used for anything!!\n",
    "query = \"\"\"\n",
    "SELECT retweeted_status.id AS id, \n",
    "       retweeted_status.text as text,\n",
    "       user.id AS user\n",
    "FROM tweets \n",
    "WHERE retweeted_status IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = int(df.rdd.getNumPartitions() * 4)\n",
    "\n",
    "Tweet = Row('id', 'text', 'user', 'lang')\n",
    "\n",
    "tweets = df.rdd \\\n",
    "           .repartition(partitions) \\\n",
    "           .map(lambda x: x.asDict(True)) \\\n",
    "           .map(lambda d: {**d, 'lang': confident_lang(d['text'])}) \\\n",
    "           .filter(lambda d: d['lang'] is not None) \\\n",
    "           .map(lambda d: Tweet(*d.values())) \\\n",
    "           .toDF() \\\n",
    "           .cache()\n",
    "\n",
    "tweets.createOrReplaceTempView('tweets')\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "with t as (\n",
    "SELECT \n",
    "    id, \n",
    "    first(lang), \n",
    "    count(*) as count, \n",
    "    collect_set(user) as users \n",
    "FROM tweets\n",
    "GROUP BY id\n",
    ")\n",
    "SELECT *\n",
    "FROM t\n",
    "WHERE count > 1\n",
    "\"\"\"\n",
    "\n",
    "tweet_users = spark.sql(query).repartition(96).cache()\n",
    "tweet_users.createOrReplaceTempView('tweet_users')\n",
    "tweet_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_users.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql.types import ArrayType, LongType\n",
    "\n",
    "def perms(a):\n",
    "    p = permutations(a, 2)\n",
    "    return list(p)\n",
    "\n",
    "spark.udf.register('perms', perms,  ArrayType(ArrayType(LongType())))\n",
    "\n",
    "query = \"\"\"\n",
    "with t as (\n",
    "SELECT explode(perms(users)) as pairs\n",
    "FROM tweet_users\n",
    ")\n",
    "SELECT pairs[0] as a, pairs[1] as b\n",
    "FROM t\n",
    "\"\"\"\n",
    "\n",
    "user_pairs = spark.sql(query)\n",
    "\n",
    "\n",
    "# user_pairs.repartition(200, 'a', 'b')\n",
    "\n",
    "for up in user_pairs.toLocalIterator():\n",
    "    pass\n",
    "# user_edges = spark.sql(query).toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(query).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "with tt as (\n",
    "with t as (\n",
    "SELECT COUNT(*) as lang_counts, user, lang\n",
    "FROM tweets\n",
    "GROUP BY user, lang\n",
    ")\n",
    "SELECT SUM(lang_counts) OVER (partition by user) as retweets,\n",
    "       lang_counts, \n",
    "       lang,\n",
    "       user\n",
    "FROM t\n",
    ")\n",
    "SELECT map_from_arrays(collect_list(lang), collect_list(lang_counts / retweets)) as langs,\n",
    "       FIRST(retweets) as retweets,\n",
    "       user\n",
    "FROM tt\n",
    "GROUP BY user\n",
    "\"\"\"\n",
    "\n",
    "user_info = spark.sql(query) \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: (r.user, r.asDict())) \\\n",
    "    .mapValues(create_user) \\\n",
    "    .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "\n",
    "for user, (lang, count) in user_info.items(): \n",
    "    G.add_node(user, lang = lang, count = count)\n",
    "\n",
    "for row in user_edges:\n",
    "    G.add_edge(row.a, row.b, weight=row.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegressionData = Row('id', 'lang', 'user', 'user_lang', 'engaged', 'cat_engaged', 'net_size')\n",
    "\n",
    "bc_ui = spark.sparkContext.broadcast(user_info)\n",
    "bc_un = spark.sparkContext.broadcast(user_networks)\n",
    "\n",
    "reg_data = tweet_users \\\n",
    "    .flatMap(lambda d: get_stats_for_users(bc_ui.value, bc_un.value, d['id'], d['lang'], d['users'])) \\\n",
    "    .map(lambda t: RegressionData(*t)) \\\n",
    "    .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_ui = spark.sparkContext.broadcast(user_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data = pd.DataFrame(reg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "edges = np.array([len(v) for v in user_networks.values()])\n",
    "sns.distplot(edges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "network.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
